# **Jieba+Sklearn** 用法

核心是用这两个工具完成**海量跨境关键词的语义聚类**——把杂乱的TikTok/跨境关键词按「语义/场景/痛点」自动分成主题组，替代人工分类的低效工作，刚好贴合你处理海量关键词、落地内容挖掘的核心需求。

结合你的TikTok跨境业务场景，我会从**核心逻辑+实操代码+结果落地**三部分讲透，代码是**开箱即用**的，适配中英文混合的跨境关键词（如TikTok小店关键词、跨境选品SEO），还会解决Jieba处理海外专有词的核心问题，最后告诉你聚类结果怎么直接对接内容挖掘。

### 一、Jieba+Sklearn 做关键词聚类的核心逻辑

针对**海量关键词列表**（不是长文本），核心是「把文字转成计算机能识别的数值，再按数值相似度分组」，全程4步，专为关键词短文本优化，贴合跨境场景：

```plain
关键词数据清洗 → 
Jieba分词（加跨境自定义词典，避免拆错专有词） → 
Sklearn TF-IDF向量化（短文本专属） → 
Sklearn KMeans聚类（按语义分组）
```

#### 各工具的核心作用

- **Jieba**：中文分词工具，解决「TikTok小店关键词优化」拆成「TikTok/小店/关键词/优化」的问题，**重点是自定义词典**（避免把TikTok、小店、选品等跨境词拆错）；
- **Sklearn**：Python机器学习库，用两个核心函数完成后续工作：

- - `TfidfVectorizer`：向量化，把分词后的关键词转成「语义数值矩阵」（比如“TikTok小店”和“TikTok商品卡”的数值相似度高，会被归为一组）；
  - `KMeans`：聚类算法，按数值相似度把关键词分成N个主题组，N可自动计算或按业务手动指定（比如按「小店/直播/选品」3个场景定N=3）。

### 二、全程实操代码（开箱即用，适配跨境关键词）

#### 前置准备

1. 安装所需库（终端执行，一行搞定）：

```bash
pip install jieba scikit-learn pandas numpy matplotlib wordcloud
```

1. 准备你的关键词数据：把海量关键词整理成**CSV/Excel文件**，只保留1列，列名建议为`keyword`，示例如下（TikTok跨境关键词）：

| keyword                |
| ---------------------- |
| TikTok小店关键词优化   |
| TikTok怎么找精准关键词 |
| 跨境选品关键词分析     |
| TikTok直播关键词布局   |
| 免费TikTok关键词工具   |
| 跨境小店SEO关键词      |

#### 核心代码（带详细注释，替换文件路径即可运行）

代码解决了**跨境关键词的3个核心问题**：① 自定义词典避免专有词拆错；② 适配中英文混合关键词；③ 自动计算最优聚类数（不用手动猜N）；④ 输出聚类结果+提取每组核心词（直接对接内容选题）。

```python
# 导入所需库
import jieba
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from wordcloud import WordCloud
import re

# -------------------------- 第一步：配置Jieba，解决跨境专有词分词问题 --------------------------
# 1. 添加跨境自定义词典（核心！避免Jieba把TikTok、小店、选品等词拆错，可根据你的业务继续补充）
custom_words = [
    "TikTok", "TikTok小店", "TikTok直播", "跨境电商", "关键词工具", 
    "跨境选品", "SEO", "小店流量", "直播布局", "选品分析"
]
for word in custom_words:
    jieba.add_word(word, freq=1000)  # freq设高，优先匹配

# 2. 定义停用词表（过滤无意义词，保留痛点词如“怎么/如何”，贴合内容挖掘）
stopwords = set([
    "的", "了", "呢", "啊", "吧", "哦", "呀", "之", "与", "和", "及"
])

# -------------------------- 第二步：数据加载与预处理 --------------------------
# 替换为你的关键词文件路径（CSV/Excel都可以，这里以CSV为例）
df = pd.read_csv("your_keywords.csv", encoding="utf-8")
# 数据清洗：去重、去空值、去除特殊字符（适配跨境关键词的/、-、空格等）
df = df.dropna(subset=["keyword"]).drop_duplicates(subset=["keyword"])
df["keyword"] = df["keyword"].apply(lambda x: re.sub(r"[\/\-\s\·\，\。\！\？]", "", str(x)))
keywords = df["keyword"].tolist()  # 转为列表，方便后续处理

# -------------------------- 第三步：Jieba分词（适配中英文混合关键词） --------------------------
def jieba_cut(text):
    """自定义分词函数：分词+过滤停用词"""
    # 分词（精准模式，适合短文本关键词）
    words = jieba.lcut(text, cut_all=False)
    # 过滤：停用词+单字（保留英文/数字，如TikTok、SEO）
    words_filtered = [w for w in words if w not in stopwords and len(w) > 1 or re.match(r"[A-Za-z0-9]+", w)]
    return " ".join(words_filtered)

# 对所有关键词分词，得到分词后的文本列表
keywords_cut = [jieba_cut(kw) for kw in keywords]

# -------------------------- 第四步：Sklearn TF-IDF向量化（短文本专属优化） --------------------------
tfidf = TfidfVectorizer(
    max_features=2000,  # 保留TOP2000高频特征，适配海量关键词
    ngram_range=(1, 2),  # 保留单词+双词（如“关键词+工具”，更贴合语义）
    lowercase=False,  # 不转小写，保留TikTok、SEO等英文专有词的大小写
    token_pattern=r"[A-Za-z0-9]+|[^\sA-Za-z0-9]+"  # 正确分割中英文混合词
)
# 把分词后的文本转为TF-IDF数值矩阵
tfidf_matrix = tfidf.fit_transform(keywords_cut)
print(f"TF-IDF向量化完成，矩阵维度：{tfidf_matrix.shape}")  # 输出：样本数×特征数

# -------------------------- 第五步：自动计算最优聚类数k（避免手动猜值） --------------------------
# 方法1：肘部法则（看拐点，拐点处为最优k）
inertias = []
k_range = range(2, 11)  # 测试k=2到10，可根据你的关键词量调整
for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init="auto")
    kmeans.fit(tfidf_matrix)
    inertias.append(kmeans.inertia_)

# 绘制肘部图，直观看最优k
plt.figure(figsize=(8, 4))
plt.plot(k_range, inertias, marker="o")
plt.xlabel("聚类数k")
plt.ylabel("惯性值（Inertia）")
plt.title("肘部法则-最优聚类数选择")
plt.grid(True)
plt.show()

# 方法2：轮廓系数（可选，系数越接近1，聚类效果越好）
# sil_scores = [silhouette_score(tfidf_matrix, KMeans(n_clusters=k, random_state=42, n_init="auto").fit_predict(tfidf_matrix)) for k in k_range]
# print("轮廓系数：", sil_scores)

# -------------------------- 第六步：Sklearn KMeans聚类（用最优k，也可手动指定） --------------------------
# 替换为你从肘部图看到的最优k（比如k=4），或按业务手动指定（如按场景定k=3）
BEST_K = 4
kmeans = KMeans(n_clusters=BEST_K, random_state=42, n_init="auto")
df["cluster_label"] = kmeans.fit_predict(tfidf_matrix)  # 给每个关键词加聚类标签

# -------------------------- 第七步：提取每组核心词+可视化+结果保存 --------------------------
def get_cluster_core_words(cluster_label, tfidf, kmeans, top_n=10):
    """提取每个聚类的TOP-N核心词（按TF-IDF权重）"""
    # 找到该聚类的中心向量
    center = kmeans.cluster_centers_[cluster_label]
    # 按权重排序，取TOP-N
    top_indices = center.argsort()[-top_n:][::-1]
    # 映射为对应的词
    core_words = [tfidf.get_feature_names_out()[i] for i in top_indices]
    return core_words

# 遍历每个聚类，输出结果+生成词云
for label in range(BEST_K):
    cluster_data = df[df["cluster_label"] == label]
    cluster_keywords = cluster_data["keyword"].tolist()
    core_words = get_cluster_core_words(label, tfidf, kmeans, top_n=10)
    # 输出聚类结果
    print(f"\n===== 聚类{label+1}（共{len(cluster_keywords)}个关键词） =====")
    print(f"核心词：{', '.join(core_words)}")
    print(f"代表关键词：{cluster_keywords[:5]}")  # 输出前5个代表关键词

    # 生成该聚类的词云（直观看核心主题，可选）
    wc = WordCloud(
        font_path="simhei.ttf",  # 解决中文显示问题，需有黑体字库
        width=400, height=300, background_color="white"
    ).generate(" ".join(cluster_keywords))
    plt.figure(figsize=(6, 4))
    plt.imshow(wc)
    plt.axis("off")
    plt.title(f"聚类{label+1}词云")
    plt.show()

# 保存最终聚类结果到CSV（方便后续整理词库+内容挖掘）
df.to_csv("keywords_cluster_result.csv", encoding="utf-8-sig", index=False)
print("\n聚类结果已保存为：keywords_cluster_result.csv")
```

### 三、关键配置说明（针对跨境关键词的优化，必看）

1. **自定义词典**：代码中`custom_words`必须根据你的业务补充，比如做东南亚市场加“印尼语”“东南亚小店”，做直播加“直播口播”“小黄车”，**这是分词准确的核心**，否则Jieba会把“TikTok小店”拆成“TikTok”“小”“店”，影响后续聚类；
2. **停用词表**：刻意保留了“怎么/如何/解决/避免”等痛点词，因为这些词是你后续挖掘用户需求的关键，不用过滤；
3. **TF-IDF参数**：`ngram_range=(1,2)`是短文本关键词的关键，能识别“关键词工具”“小店流量”这类组合词，让聚类更贴合语义；
4. **聚类数k**：优先用**肘部图的拐点**选k，比如拐点在k=4，就设`BEST_K=4`；也可按业务手动指定，比如你想按「赛道核心词/长尾痛点词/场景落地词/福利需求词」分4类，直接设k=4即可。

### 四、聚类结果如何直接落地「内容挖掘」（核心衔接你的业务）

代码运行后，你会得到**带聚类标签+核心词的关键词表**，这一步是把技术结果转化为内容挖掘的实际价值，直接对接你之前的内容挖掘逻辑，分3步落地：

#### 步骤1：给每个聚类组定「内容主题」

根据聚类的**核心词+代表关键词**，给每个组起一个贴合业务的主题名，比如：

- 聚类1核心词：TikTok、小店、关键词、优化 → 主题：**TikTok小店关键词布局**
- 聚类2核心词：TikTok、直播、布局、口播 → 主题：**TikTok直播关键词实操**
- 聚类3核心词：跨境、选品、分析、蓝海 → 主题：**跨境选品关键词挖掘**
- 聚类4核心词：关键词、工具、免费、实操 → 主题：**TikTok关键词工具使用**

#### 步骤2：从每组中提取「需求类型词」

在每个聚类组内，筛选出**痛点词/场景词/福利词**，直接对接用户需求挖掘，比如：

- 组1（TikTok小店关键词）：提取痛点词（找不准、流量低、排名上不去）、场景词（标题、商品卡、SEO）；
- 组4（关键词工具）：提取福利词（免费、清单、教程）、痛点词（付费踩坑、不会用）。

#### 步骤3：直接生成「内容选题方向」

用「聚类主题+需求类型词」拼接，快速生成批量选题，不用凭空想，比如：

- 组1+痛点词：《TikTok小店关键词找不准？3步优化，流量翻倍》
- 组2+场景词：《TikTok直播关键词布局：标题/口播/小黄车，3个位置精准植入》
- 组4+福利词：《5个免费TikTok关键词工具，跨境卖家亲测，不用付费踩坑》

### 五、进阶优化（贴合你的关键词工具产品）

如果想把这个「Jieba+Sklearn聚类功能」集成到你的**TikTok关键词工具**中，做产品功能升级，有3个轻量优化方向，开发成本低，用户价值高：

1. **可视化聚类结果**：在工具中做「关键词聚类云图」，用户上传词库后，直接生成主题聚类云，直观看到核心主题；
2. **按场景自动分类**：预设「小店/直播/选品/工具」等跨境场景标签，聚类后自动匹配场景，用户可直接导出「场景化词库」；
3. **选题推荐联动**：聚类后，根据每组核心词，自动生成3-5个TikTok短视频选题，实现「找词→聚类→做内容」的一站式服务（对接你之前的内容挖掘逻辑）。

### 六、避坑指南（新手易踩的3个坑）

1. **分词错误**：没加自定义词典，导致跨境专有词被拆错→**必加自定义词典**，并把高频跨境词的`freq`设高；
2. **聚类结果混乱**：关键词清洗不彻底，有大量生僻词/无意义词→**严格去重、去空、去特殊字符**，只保留有实际语义的关键词；
3. **最优k选不准**：k太小导致主题混杂，k太大导致主题过细→**结合肘部法则+业务场景**，比如肘部图拐点k=5，你可根据业务合并为4个核心场景。

### 总结

Jieba+Sklearn是处理海量跨境关键词的**高效自动化工具**，核心价值是**替代人工完成语义聚类**，让杂乱的词库变成有逻辑的「主题化词库」，而聚类结果的最终落地，还是要回到你之前的核心逻辑——**从聚类主题中挖用户需求，再转化为内容选题**。

整个流程形成闭环：**海量关键词→Jieba+Sklearn聚类→主题化词库→需求挖掘→内容选题→数据反哺词库**，完全贴合你TikTok跨境营销+关键词工具的业务需求。

如果有具体的关键词样本（比如100个TikTok跨境关键词），我可以帮你**实操运行代码，输出完整的聚类结果+核心词+5个TikTok短视频选题**，直接套用，需要吗？